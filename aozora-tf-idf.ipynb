{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "# ライブラリのダウンロード\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# import urllib\n",
    "import urllib.request\n",
    "import sys\n",
    "import zipfile\n",
    "import csv\n",
    "import os\n",
    "import pyprind\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数\n",
    "csvUrl= \"https://www.aozora.gr.jp/index_pages/list_person_all_extended_utf8.zip\"\n",
    "csvFileName = \"list_person_all_extended_utf8\"\n",
    "autherName=[\"芥川竜之介\",\"太宰治\",\"夏目漱石\",\"宮沢賢治\"]\n",
    "autherID = [\"000879\",\"000035\",\"000148\",\"000081\"]\n",
    "dataPath = \"aozora\"\n",
    "trainPath = \"train\"\n",
    "testPath = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('list_person_all_extended_utf8.zip',\n",
       " <http.client.HTTPMessage at 0x1c6b63e5e48>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 青空文庫の作者情報csvをダウンロード(Zip)\n",
    "urllib.request.urlretrieve(csvUrl,csvFileName+\".zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zipを展開する\n",
    "with zipfile.ZipFile(csvFileName+\".zip\") as existing_zip:\n",
    "    existing_zip.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csvを読み込む\n",
    "csvReader = \"\"\n",
    "with open(\"./\"+csvFileName+\".csv\",encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    csvReader = [row for row in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 保存するフォルダーの作成\n",
    "if not os.path.exists(dataPath):\n",
    "    os.mkdir(dataPath)\n",
    "for auther in autherName:\n",
    "    if not os.path.exists(dataPath+\"/\"+auther):\n",
    "        os.mkdir(dataPath+\"/\"+auther)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:19\n"
     ]
    }
   ],
   "source": [
    "# 青空文庫から作品をまとめてダウンロードするので時間がかかります！\n",
    "humanIdOrder = csvReader[0].index(\"人物ID\")\n",
    "textFileOrder = csvReader[0].index(\"XHTML/HTMLファイルURL\")\n",
    "workNameOrder = csvReader[0].index(\"作品名\")\n",
    "roleFlag = csvReader[0].index(\"役割フラグ\")\n",
    "pbar = pyprind.ProgBar(len(csvReader))\n",
    "\n",
    "# 青空文庫のファイルを保存する\n",
    "def saveAozoraFile(auther,work,url):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, 'lxml')\n",
    "    elem = soup.find(class_='main_text')\n",
    "    if elem==None:\n",
    "        return\n",
    "    rp_all = elem.find_all('rp')\n",
    "    for rp in rp_all:\n",
    "        rp.extract()\n",
    "    rt_all = elem.find_all('rt')\n",
    "    for rt in rt_all:\n",
    "        rt.extract()\n",
    "\n",
    "    text = elem.get_text()\n",
    "    with open(dataPath+\"/\"+auther+\"/\"+work+\".txt\",\"w\",encoding='utf-8')as f:\n",
    "        f.write(text)\n",
    "\n",
    "for row in csvReader:\n",
    "    pbar.update()\n",
    "    if autherID.count(row[humanIdOrder])!=0 and row[roleFlag]==\"著者\" and re.match(\"https?://www.aozora.gr.jp/cards/\",row[textFileOrder]) and \"_\" in row[textFileOrder]:\n",
    "        selectAuther = autherID.index(row[humanIdOrder])\n",
    "        if not os.path.exists(dataPath+\"/\"+autherName[selectAuther]+\"/\"+row[workNameOrder]+\".txt\"):\n",
    "            saveAozoraFile(autherName[selectAuther],row[workNameOrder],row[textFileOrder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニングデータを読み込む\n",
    "df = pd.DataFrame()\n",
    "# testとtrainのフォルダを探索する\n",
    "for s in ([dataPath]):\n",
    "    #  posとnegのフォルダを探索する\n",
    "    for l,i in zip(autherName,range(len(autherName))):\n",
    "        # フォルダ名を結合する\n",
    "        path = os.path.join(\"./\", s, l)\n",
    "        # ファイルを開いてpandasのDataFrameに追加する\n",
    "        for j,file in enumerate(sorted(os.listdir(path))):\n",
    "            if j>=100:\n",
    "                break\n",
    "            with open(os.path.join(path, file),'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, i]],ignore_index=True)\n",
    "df.columns = ['ドキュメント', '作者ID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シャッフルする\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ドキュメント</th>\n",
       "      <th>作者ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>\\nはしがき\\n\\n\\n　もの思う葦という題名にて、日本浪曼派の機関雑誌におよそ一箇年ほどつ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>\\n\\nこのみちの醸すがごとく\\n\\n粟葉などひかりいでしは\\n\\nひがしなる山彙の上に\\n...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>\\n\\n洪積の台のはてなる\\n\\n一ひらの赤き粘土地\\n\\n\\n桐の群白くひかれど\\n\\n枝...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>\\n\\n\\n\\n（わが陋屋には、六坪ほどの庭があるのだ。愚妻は、ここに、秩序も無く何やらかや...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>\\n\\n\\n\\n　さっきから松原を通ってるんだが、松原と云うものは絵で見たよりもよっぽど長い...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>\\n\\n\\n\\n　　　　　　　　一\\n\\n　小野の小町、几帳の陰に草紙を読んでいる。そこへ突...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>\\n\\n\\n\\n　芸術家というものは、つくづく困った種族である。鳥籠一つを、必死にかかえて、...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>\\n\\n\\n\\n　私はこの大阪で講演をやるのは初めてであります。またこういう大勢の前に立つの...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>\\n\\n\\n\\n　一人の遊蕩の子を描写して在るゆえを以て、その小説を、デカダン小説と呼ぶのは...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>\\n\\n\\n\\n「煤煙」が朝日新聞に出て有名になつてから後間もなくの話であるが、著者は夫を単...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ドキュメント  作者ID\n",
       "132  \\nはしがき\\n\\n\\n　もの思う葦という題名にて、日本浪曼派の機関雑誌におよそ一箇年ほどつ...     1\n",
       "309  \\n\\nこのみちの醸すがごとく\\n\\n粟葉などひかりいでしは\\n\\nひがしなる山彙の上に\\n...     3\n",
       "341  \\n\\n洪積の台のはてなる\\n\\n一ひらの赤き粘土地\\n\\n\\n桐の群白くひかれど\\n\\n枝...     3\n",
       "196  \\n\\n\\n\\n（わが陋屋には、六坪ほどの庭があるのだ。愚妻は、ここに、秩序も無く何やらかや...     1\n",
       "246  \\n\\n\\n\\n　さっきから松原を通ってるんだが、松原と云うものは絵で見たよりもよっぽど長い...     2\n",
       "60   \\n\\n\\n\\n　　　　　　　　一\\n\\n　小野の小町、几帳の陰に草紙を読んでいる。そこへ突...     0\n",
       "155  \\n\\n\\n\\n　芸術家というものは、つくづく困った種族である。鳥籠一つを、必死にかかえて、...     1\n",
       "261  \\n\\n\\n\\n　私はこの大阪で講演をやるのは初めてであります。またこういう大勢の前に立つの...     2\n",
       "141  \\n\\n\\n\\n　一人の遊蕩の子を描写して在るゆえを以て、その小説を、デカダン小説と呼ぶのは...     1\n",
       "214  \\n\\n\\n\\n「煤煙」が朝日新聞に出て有名になつてから後間もなくの話であるが、著者は夫を単...     2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 読み込めているかチェック\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日 の 中野 の 天気 は 晴れ です 。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mecabの環境が必要です\n",
    "# Mecabが動くかのテスト\n",
    "import MeCab\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "text = mecab.parse (\"今日の中野の天気は晴れです。\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mecabを使って、日本語を形態素解析する関数を定義\n",
    "# Mecabの環境が必要です\n",
    "# http://taku910.github.io/mecab/#download\n",
    "import MeCab\n",
    "\n",
    "def preprocessor(text):\n",
    "    mecab = MeCab.Tagger(\"-Owakati\")\n",
    "    text=re.sub(\"\\\\n\",\"\",text)\n",
    "    return mecab.parse(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# すべてのドキュメントに関数を使う\n",
    "df['ドキュメント'] = df['ドキュメント'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ドキュメント</th>\n",
       "      <th>作者ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>はし が き 　 もの 思う 葦 という 題名 にて 、 日本 浪 曼派 の 機関 雑誌 に...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>この みち の 醸す が ごとく 粟 葉 など ひかり い でし は ひがし なる 山 彙 ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>洪 積 の 台 の はて なる 一 ひ ら の 赤き 粘土 地 桐 の 群 白く ひか れ ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>（ わが 陋屋 に は 、 六 坪 ほど の 庭 が ある の だ 。 愚妻 は 、 ここ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>さっき から 松原 を 通っ てる ん だ が 、 松原 と 云う もの は 絵 で 見...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>一 　 小野 の 小町 、 几帳 の 陰 に 草紙 を 読ん...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>芸術 家 という もの は 、 つくづく 困っ た 種族 で ある 。 鳥 籠 一つ を...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>私 は この 大阪 で 講演 を やる の は 初めて で あり ます 。 また こうい...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>一 人 の 遊蕩 の 子 を 描写 し て 在る ゆえ を以て 、 その 小説 を 、 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>「 煤煙 」 が 朝日新聞 に 出 て 有名 に な つて から 後 間もなく の 話 で ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ドキュメント  作者ID\n",
       "132  はし が き 　 もの 思う 葦 という 題名 にて 、 日本 浪 曼派 の 機関 雑誌 に...     1\n",
       "309  この みち の 醸す が ごとく 粟 葉 など ひかり い でし は ひがし なる 山 彙 ...     3\n",
       "341  洪 積 の 台 の はて なる 一 ひ ら の 赤き 粘土 地 桐 の 群 白く ひか れ ...     3\n",
       "196  （ わが 陋屋 に は 、 六 坪 ほど の 庭 が ある の だ 。 愚妻 は 、 ここ ...     1\n",
       "246  　 さっき から 松原 を 通っ てる ん だ が 、 松原 と 云う もの は 絵 で 見...     2\n",
       "60   　 　 　 　 　 　 　 　 一 　 小野 の 小町 、 几帳 の 陰 に 草紙 を 読ん...     0\n",
       "155  　 芸術 家 という もの は 、 つくづく 困っ た 種族 で ある 。 鳥 籠 一つ を...     1\n",
       "261  　 私 は この 大阪 で 講演 を やる の は 初めて で あり ます 。 また こうい...     2\n",
       "141  　 一 人 の 遊蕩 の 子 を 描写 し て 在る ゆえ を以て 、 その 小説 を 、 ...     1\n",
       "214  「 煤煙 」 が 朝日新聞 に 出 て 有名 に な つて から 後 間もなく の 話 で ...     2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "# 日本語のstopを定義\n",
    "# 出典：https://qiita.com/chamao/items/7edaba62b120a660657e\n",
    "def download_stopwords(path):\n",
    "    url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "    if os.path.exists(path):\n",
    "        print('File already exists.')\n",
    "    else:\n",
    "        print('Downloading...')\n",
    "        # Download the file from `url` and save it locally under `file_name`:\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "\n",
    "def create_stopwords(file_path):\n",
    "    stop_words = []\n",
    "    for w in open(path, \"r\",encoding='utf-8'):\n",
    "        w = w.replace('\\n','')\n",
    "        if len(w) > 0:\n",
    "            stop_words.append(w)\n",
    "    return stop_words    \n",
    "\n",
    "path = \"stop_words.txt\"\n",
    "download_stopwords(path)\n",
    "stop = create_stopwords(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainとtestデータに分ける\n",
    "X_train = df.loc[:300, 'ドキュメント'].values\n",
    "y_train = df.loc[:300, '作者ID'].values\n",
    "X_test = df.loc[300:, 'ドキュメント'].values\n",
    "y_test = df.loc[300:, '作者ID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今日', '中野', '天気', '美しい']\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# 名詞と形容詞のだけに絞り、形容詞は原形にする\n",
    "def tokenizer_porter(text):\n",
    "    mecab = MeCab.Tagger(\"-Ochasen\")\n",
    "    text=mecab.parseToNode(text)\n",
    "    newText = []\n",
    "    while text:\n",
    "        featureSplit = text.feature.split(\",\")\n",
    "        if featureSplit[0]==\"名詞\" or featureSplit[0]==\"形容詞\":\n",
    "            newText.append(featureSplit[6])\n",
    "        text=text.next\n",
    "    return newText\n",
    "\n",
    "print(tokenizer_porter(\"今日 の 中野 の 天気 は 美しく 晴れ です 。 \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5分割交差検定でロジスティック回帰モデルの最適なパラメータ集合を求める\n",
    "\n",
    "# 日本語のtfidfの参照\n",
    "# https://analytics-note.xyz/machine-learning/bow-tfidf-one-character/\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None,\n",
    "                       token_pattern='(?u)\\\\b\\\\w+\\\\b',\n",
    "                       max_features=5000)\n",
    "\n",
    "# グリッドサーチを定義\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "\n",
    "# pipelineに入れる\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "# 注意！！gs_lr_tfidfのn_jobs=-1でマルチプロセス処理を最大で実行。\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:  4.3min finished\n",
      "C:\\Users\\gooda\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\gooda\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\gooda\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=5000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...e, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['あそこ', 'あたり', 'あちら', 'あっち', 'あと', 'あな', 'あなた', 'あれ', 'いくつ', 'いつ', 'いま', 'いや', 'いろいろ', 'うち', 'おおまか', 'おまえ', 'おれ', 'がい', 'かく', 'かたち', 'かやの', 'から', 'がら', 'きた', 'くせ', 'ここ', 'こっち', 'こと', 'ごと', 'こちら', 'ごっちゃ', 'これ', 'これら', 'ごろ', 'さまざま', 'さらい...kenizer_porter at 0x000001C6BE2C20D0>], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# この処理は数分かかる恐れがあります!\n",
    "# トレーニング開始\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 100.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x000001C6BE2C2598>} \n",
      "CV Accuracy: 0.859\n"
     ]
    }
   ],
   "source": [
    "# ベストなパラメーター集合とそのスコア\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.877\n"
     ]
    }
   ],
   "source": [
    "# 90%程度の正解率で肯定的か否定的かを判定できる\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
